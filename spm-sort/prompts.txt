We are continuing the SPM project (Distributed Out-of-Core MergeSort).  
The work so far covers Phases 3 â†’ 5 of the official checklist.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“˜ Project structure & design
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Core record format: [u64 key][u32 len][len bytes payload]  
  Defined in record.hpp â†’ do NOT modify it.  
  read_record() / write_record() are used everywhere.

â€¢ Shared data structures moved to include/data_structure.hpp  
  â”€ Item { key, payload }
  â”€ TempReader { reads one sorted run file }
  â”€ HeapNode { key, run_idx, comparator for min-heap }

â€¢ Sequential baseline (Phase 3)
  â”€ Loads the entire file (< MEMORY_CAP) in RAM, sorts via std::sort, writes sorted_records.bin.

â€¢ Out-of-core single-node (Phase 4)
  â”€ Reads until ~60 % of RAM (MEMORY_CAP = 2 GiB test / â‰¤ 32 GiB cluster).  
  â”€ Sorts each chunk, spills run_i.bin to tmp/.  
  â”€ Performs a k-way merge using a min-heap of HeapNodes.  
  â”€ Ensures the â€œtailâ€ chunk is always flushed (removed the else branch).  
  â”€ Cleans temporary runs after merge.

â€¢ Verify tool (verify.cpp)
  â”€ Streams records.bin & sorted_records.bin.  
  â”€ Checks keys are non-decreasing.  
  â”€ Confirms equal record count, payload bytes, XOR hash, and SUM hash.  
  â”€ Pure standard C++20, no __int128, header-only spdlog.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âš™ï¸ Phase 5 â€“ OpenMP single-node
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Goal: â€œParallel sort per run + parallel multiway merge; identical output; > 1.5Ã— speedup on 8 threads.â€

â€¢ New file src/ooc_omp.cpp  
  Reuses all sequential logic but adds OpenMP at two hotspots:
  1ï¸âƒ£ **Parallel sort per run**  
      - Split each in-RAM run into T blocks (T = threads).  
      - #pragma omp parallel for â†’ std::sort each block by key.  
      - Sequential k-way merge of T subranges â†’ identical order.
  2ï¸âƒ£ **Parallel grouped merge**  
      - Partition temp runs into G groups (~threads).  
      - Each thread merges its group sequentially (reusing merge engine).  
      - One final single-thread merge of intermediates.  
      - Cleans up inter_*.bin files.

â€¢ Build flags  
  - Compile ooc_omp with `-fopenmp -std=c++20 -O3 -Wall -Wextra -pedantic`.  
  - Example Makefile builds: seq_sort, ooc_omp, verify.

â€¢ Runtime
  export OMP_NUM_THREADS=8  
  ./bin/ooc_omp  
  then ./bin/verify â†’ should report OK (same data, sorted).

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”– Key implementation details remembered
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ MEMORY_CAP guard enforced in both versions.  
â€¢ Tail chunk always sorted & spilled.  
â€¢ Removed else after spill to avoid record loss.  
â€¢ run_index uses size_t (no overflow).  
â€¢ K-way merge uses std::priority_queue<HeapNode,â€¦,greater<>>.  
â€¢ Temporary files auto-deleted.  
â€¢ Output format identical to sequential version.  
â€¢ Expected acceptance: identical output, > 1.5Ã— speedup on 8 threads.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Next steps
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
From this state we can move on to:
  â€¢ Phase 6 â€“ FastFlow pipeline version (parallel with FF farm/pipeline)
  â€¢ Phase 7 â€“ MPI + OpenMP/FastFlow distributed version
  â€¢ Add performance instrumentation & plots for the report.