INTRODUCTION

In this project I have tried to solve the problem of sorting large datasets that cannot fit in memory. The problem appears often in databases, search engines, and data processing systems. When data is larger than available RAM, we must use external sorting techniques. These techniques read data in segments, sort each segment in parallel, and then merge them on disk.

I have started with a function which checks the input size from the storage. If the data fits in memory, it sorts everything at once in parallel. If the data is too large, it divides the input into segments. Each segment is bounded by DISTRIBUTION_CAP. The program sorts each segment and writes it to disk. Finally, it performs a k-way merge to produce the sorted output.

For the implementation I have used structured parallel programming concepts from the course. I have implemented the solution using two frameworks: OpenMP and FastFlow. Both solutions follow the same pattern. An emitter stage reads the input and creates segments. Each segment is divided into DEGREE^** slices. Each slice fits in the L1 cache of the processor. Worker stages receive these slices and sort them in parallel. This is map-style data parallelism. Writer stages flush the sorted data to intermediate files of out-of-core sorting. Finally, a collector stage performs the final k-way merge.

The implementation of OpenMP uses manual thread management with SafeQueue that relies on std::mutex and std::condition_variable for synchronization. Each queue has a tunable max size for backpressure control. When the queue is full, the producer thread blocks using condition_variable.wait until consumers free space. Similarly, when the queue is empty, consumer threads block until producers add items. The formula ${DEGREE*WORKERS} determines the queue capacity for dynamic load distribution. I also used std::mutex with std::lock_guard to protect the shared run_paths vector when writers add completed segments.

On the other hand FastFlow provides set_scheduling_ondemand for automatic load balancing. In this mode workers pull tasks when they are ready. The backp-ressure is handled by FastFlow's internal bounded channels between emitter and workers. When all workers are busy their input queues become full. The emitter's ff_send_out then blocks automatically. This slows down the reading from the file. The FastFlow runtime manages these channels internally. This removes the need for manual queues, condition variables, and queue size tuning which makes the code much simpler and shorter.

The final version with MPI+FastFlow hybrid version. The master process (rank 0) distributes segments to worker nodes. Each worker node runs a FastFlow farm internally. After sorting, workers send results back to the master. The master performs the final global merge. This approach extends the solution to multi-node systems.

One important optimization was using CompactPayload pointers instead of nested vectors. The original implementation used std::vector<Item> where each Item contained std::vector<uint8_t>. This created 24 bytes of overhead per record. By using pointers (8 bytes), I saved 16 bytes per record. For millions of records, this reduces memory consumption significantly.

** PUT THIS IN THE PROBLEM SECTION **
I tested both OpenMP and FastFlow implementations. OpenMP achieved approximately 5 seconds. FastFlow achieved approximately 6 seconds. The performance is very similar. However, I had to write much more code for OpenMP. I needed explicit queues, mutexes, and careful synchronization. FastFlow gave similar results with less coordination code. This shows that FastFlow is easier to use.

Right now the FastFlow gives similar performance like OpenMP but I think with more optimization and tests the results can be improved. For example, we can suggest an efficient algorithm to choose DISTRIBUTION_CAP and DEGREE based on input size and system configuration. We can also try lock-free mechanism for the safe-queues. The current implementation uses array of structures which can be replaced with structures of array and tested. Right now emitter and collector stages are sequentially(discussed later in brief) reading and writing the data in/out from/to a file, which can be parallelized and tested. So, we can say that this implementation is not 100% efficient but it is a good starting point which covers most of the concept and coding practices from the course.  

This project demonstrates that structured parallel patterns work well for external sorting. The map pattern over slices and the pipeline pattern provide good scalability. Both OpenMP and FastFlow deliver strong performance. The FastFlow framework simplifies development. With targeted optimizations, it should match OpenMP while keeping the code simpler.
