\documentclass[11pt,a4paper]{article}

%%%%%%%% CREATE DOCUMENT STRUCTURE %%%%%%%%
%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage{subfig}

%% Sets page size and margins
\usepackage[a4paper,top=2cm,bottom=2cm,left=1cm,right=1cm,marginparwidth=1.75cm]{geometry}
%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\graphicspath{{images/}}
\setlength{\marginparwidth}{2cm}
\usepackage[colorlinks=true, urlcolor=blue, linkcolor=black, citecolor=blue]{hyperref}
\usepackage{caption}
\usepackage{sectsty}
\usepackage{float}
\usepackage{xcolor}
\usepackage{listings}
\definecolor{darkgreen}{rgb}{0.0, 0.4, 0.0}
\pagenumbering{arabic}
\sectionfont{\Huge}

%%%%%%%% DOCUMENT %%%%%%%%
\begin{document}

%%%% Title Page
\begin{titlepage}

    \newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
    \center
    \vspace*{3cm}
    % University
    \textsc{\LARGE Universita di Pisa}\\[1cm]
    \includegraphics[width=0.2\textwidth]{unipi_logo.png}\\[1cm]
    % Document info
    \textsc{\Large Parallel and Distributed Systems}\\[0.2cm]
    \textsc{\large Professor Massimo Torquati}\\[1cm] 										% Course Code
    \HRule \\[0.8cm]
    { \huge \bfseries Project: Distributed Out-of-Core Merge Sort}\\
    { \texttt{github:\href{https://github.com/bhuiyan-chafi/spm/tree/main/spm-sort}{https://github.com/bhuiyan-chafi/spm/tree/main/spm-sort}} }
    \HRule \\[2cm]
    \large
    \emph{By: ASM Chafiullah Bhuiyan}\\
    Computer Science and Networking \\ [0.2cm]
    \emph{\textbf{Academic Year: 2025-2026}}
    \vfill
\end{titlepage}

\section{Abstract}
In this project we tried to solve the problem of sorting large datasets that cannot fit in memory. The problem appears often in databases, search engines, and data processing systems. We must adopt external sorting techniques for datasets that don't fit in MEMORY. These techniques read data in segments, sort each segment in parallel, and then merge them on disk. In my implementation I have started with a function which checks the input size from the disc and takes decision if it can be operated \texttt{IN\_MEMORY} or \texttt{MEMORY OUT-of-CORE}. In the \texttt{OOC} mode I have segmented the data into smaller parts and each segment is bounded by a constant called \texttt{DISTRIBUTION\_CAP}. The program sorts each segment and writes back on disk. Finally, it performs a k-way merge to produce the sorted output(one big binary file). For the implementation I have used structured parallel programming concepts from the course\texttt{[FastFlow, OpenMP, MPI]}. Both solutions follow the same pattern where an emitter stage reads the input and creates segments. Each segment is divided into
    {
        \renewcommand{\thefootnote}{**}
        DEGREE\footnote{\textbf{a tunable CONSTANT that can be tuned based on input size, data-type and hardware capacity. It generates tasks based on the parameters so that each worker always process inputs equal to it's private cache.}}
    }
slices. Each slice fits in the L1 cache of the \texttt{PEs}. Worker stages fetch these slices and sort them in parallel following the map-style data parallelism. If the dataset doesn't fit in memory, workers(in FastFlow) and writers(in OpenMP) writes those intermediate segments on disc and then the Collector finally merges them back into one \texttt{output.bin} file.

\subsection{The Workflow}
The implementation of OpenMP uses a manual thread management system with SafeQueue and mutex locks for synchronization. To write the intermediate results a thread-pool has been used with a tunable queue size. The formula ${DEGREE*WORKERS}$ has been used to generate number of tasks for \texttt{dynamic load distribution}, so that the workers can fetch tasks whenever they are free. This also puts back-pressure to the emitter when all the workers are busy processing tasks. On the other hand \textbf{FastFlow} provides \textbf{{set\_scheduling\_ondemand()}} for automatic load balancing. In this mode workers pull tasks when they are ready. The back-pressure is handled by FastFlow's \textbf{internal bounded channels} between emitter and workers. When all workers are busy their input queues become full and emitter's \textbf{{ff\_send\_out}} blocks them automatically. This slows down the reading from the file. \textbf{The FastFlow runtime manages these channels internally which removes the need for manual queues, condition variables, and queue size.} This makes the implementation much easier for the developer. The final \texttt{multi-node} version is built with \textbf{MPI+FastFlow} following a \texttt{master-worker} pattern. The master process (rank 0) distributes segments to worker nodes. Each worker node runs a FastFlow farm internally. After sorting, workers send results back to the master. The master performs the final global merge.

\subsection{Encountered Problems}
Even though I have discussed the problems in detail later, I will state the major ones here. The first naive problem was defining a \textbf{vector<uint\_8> payload} for the items instead of \textbf{pointer}. Professor specifically mentioned \textbf{char payload[]} for the payload, but I defined vectors as it gives flexibility for dynamic sized arrays. The result was catastrophic, and I have changed it in the later versions of OpenMP implementation(there are 6 versions of OpenMP; with each upgrade one major issue has been solved). One among the other problems was poor synchronization between the \textbf{emitter} and \textbf{workers}. The synchronization was so bad that the emitter kept reading from the input while the workers were busy with their existing tasks. Another problem was having a bad distribution system which affected the performance too much. The last major problem was putting too much pressure on the collector(in memory out-of-core phase) that it was almost paralyzed. There were other problems of poor communication between nodes in the multi-node version, but I have discussed that in a separate section later.

\subsection{Future Work}
Right now the FastFlow gives similar performance like OpenMP but I think with more optimization and tests the results can be improved. For example, we can suggest an efficient algorithm to choose \texttt{DISTRIBUTION\_CAP} and \texttt{DEGREE} based on input size and system configuration. We can also try lock-free mechanism for the safe-queues. The current implementation uses array of structures which can be replaced with structures of array and tested. Right now emitter and collector stages are sequentially(discussed later in brief) reading and writing the data in/out from/to a file, which can be parallelized and tested.So, we can say that this implementation is not 100\% efficient but it is a good starting point which covers most of the concept and coding practices from the course.

\section{Prerequisites}
As per the project requirements I have implemented two versions, \textbf{OpenMP and FastFlow for single node; MPI and FastFlow for multi-node.} Before discussing the implementation I will discuss some pre and post requisite programs. Data generation and verification are two most important part of the whole project. Since they are time consuming, to save time I have used pre-generated data by using the program \textbf{generator.cpp}(/src/generator.cpp) and to verify the data I have used \textbf{verify.cpp \& reader.cpp}. Let us discuss them first very shortly. Also a short demonstration is provided in the \texttt{readme.md} file of the github repository.

\subsection{Data Generation}
The program follows the data-structure provided by my professor in the project. It generates arbitrary number of records defined by the user. For the dynamic number of payload I have used a class \textbf{CompactPayload}, which uses \texttt{uint\_8} pointer for data storage. Then I have used two \texttt{struct} for storing and processing the records: \textbf{struct} Record\{\textbf{uint64\_t} key; \textbf{uint32\_t} len;\} which stores the 64-bit key followed by the 32-bit payload length. After reading from disk, the program converts records into \textbf{struct} Item\{\textbf{uint64\_t} key; \textbf{CompactPayload} payload;\} for in-memory processing. It stores the length \textbf{uint32\_t} at the beginning of the allocated memory block, followed by the actual payload bytes. The program uses \texttt{uniform\_int\_distribution()} for random data generation and stores them as binary file \textbf{*.bin}. The execution of all programs are described separately in the \textbf{Execution} section.

\subsection{Data Verification}
The program \textbf{verify.cpp} ensures data integrity and correctness of the sorting process using a multi-step validation approach. Instead of comparing each record key for correct sorted order, I have followed a divide \& conquer technique followed by computing a global hash. First, it computes a cumulative hash of all payload data from the input file using the FNV-1a 64-bit hash algorithm. This creates a unique fingerprint of the input data. Second, it performs basic file validation by checking that the input and output files have identical sizes and contains the same number of records. If either check fails, the verification aborts immediately. Third, the program divides the output file into equal-sized chunks based on the number of available workers. Each chunk is loaded into memory and verified independently. For each chunk, the program checks local sorted order by ensuring every key is greater than or equal to the previous key within that chunk. It also verifies global sorted order by comparing the first key of each chunk with the last key of the previous chunk. While checking the sorted order, the program simultaneously computes payload hashes for all records and accumulates them. Finally, the cumulative output hash is compared with the input hash. If both hashes match and all sorted order checks pass, the verification succeeds, confirming that the output is correctly sorted and no data was corrupted or lost during the sorting process.\\[10pt]
The reader program \textbf{reader.cpp} provides a simple visual inspection tool for verifying sorted order by printing a specified number of records from either the input or output file. The user specifies how many records to display \(e.g., 100, 1000\), and the program sequentially reads and prints each record's index, key value, and payload length. By examining the printed key values in order, users can visually confirm whether the keys are in non-decreasing order \(sorted\) or in random order \(unsorted\). This manual inspection complements the automated verification program and helps during development and debugging to spot sorting issues quickly.

\subsection{Execution Commands}
\subsubsection{Data Generation }
The program takes arbitrary number of parameters. The first command is the program name followed by payload length(8 to 256 bytes long) and number of records(1M/5M/10M or 100M). To execute use the following:
\begin{lstlisting}
    $ cd "project-root"
    $ cd src/
    $ make generator
    $ ./generator 256 1M
\end{lstlisting}
The program generates the records under \textbf{/data/rec\_*\_*.bin} file. In the programs this directory has been hard-coded for both input and output.

\subsubsection{Data Verification}
The program takes the name of the input file only. The output file \textbf{(/data/output.bin)} is universal for all type of inputs. Which means if we run two programs one after another with two different input, we have verify one before running the another one. One useful could be:
\begin{lstlisting}
    $ cd "project-root"
    $ cd src/
    $ make verify
    $ ./verify rec_1M_256.bin
\end{lstlisting}

\subsubsection{OpenMP}
The program has 6 versions, each with different problems(discussed later). But the final version \textbf{openmp.cpp} has been finalized with all the fixes and some optimizations. To run the program use the following command:
\begin{lstlisting}
    $ cd "project-root"
    $ cd src/
    $ make openmp
    $ ./openmp 1M 256 4 32
\end{lstlisting}
The first parameter \textbf{program\_name} is followed by \textbf{number of records}, \textbf{payload length}, \textbf{number of workers} and \textbf{MEMORY\_CAP}. \textbf{MEMORY\_CAP} is an important parameter which follows the \textbf{ memory out of core operation } mentioned in the project description. Actually it is very hard to test a file >32GiB in the \textbf{spmcluster}, that is why this parameter is tunable to test the behavior. If you have a file of 4GiB and you want to test if the program stops with a memory of 2GiB or not! You can simply tune the \textbf{MEMORY\_CAP} and set it to any size less than 4GiB and test the behavior.

\subsubsection{The Mapping String}
Before running FastFlow programs, my professor instructed us to execute \texttt{mapping\_string.sh} script to configure optimal thread-to-core mapping for the specific machine topology. The script uses \textbf{hwloc} tools to detect the system's CPU architecture including physical cores, logical cores, and NUMA nodes. It builds a mapping string that assigns threads to cores in topologically contiguous order rather than following the operating system's default numbering. For example, instead of sequential OS numbering like 0,1,2,3, it creates a string like 0,4,2,6,1,5,3,7 which groups cores by NUMA nodes and fills physical cores before hyperthreads. This mapping is written to FastFlow's \texttt{config.hpp} file along with the total number of logical and physical cores available. Running this script improves performance significantly because it ensures NUMA-aware thread placement which reduces cross-NUMA memory access latency, optimizes cache utilization by keeping threads on the same physical core together, prevents the OS scheduler from arbitrarily migrating threads across cores which causes cache thrashing, and intelligently handles hyperthreading by filling physical cores first before using hyperthread contexts. Without this configuration, the operating system might place communicating threads on different NUMA nodes causing 2-3x slower memory access, or move threads between cores destroying cache locality. This topology-aware placement improves FastFlow performance to some level on NUMA systems, which is critical for the tight communication patterns between emitter, workers, and collector in the farm pipeline where threads frequently exchange data through bounded channels.

\subsubsection{Sequential}
Number of workers is unused here. But still we have to put a number here only for parsing the \texttt{cli} arguments.
\begin{lstlisting}
    $ cd "project-root"
    $ cd src/
    $ make sequential
    $ ./sequential 1M 256 1 32
\end{lstlisting}

\subsubsection{FastFlow}
The FastFlow version has a similar execution command like OpenMP with same number and type of parameters:
\begin{lstlisting}
    $ cd "project-root"
    $ cd src/
    $ make farm
    $ ./farm 1M 256 4 32
\end{lstlisting}

\subsubsection{MPI+FastFlow}
This is the final version of the project. The program execution is also similar but I recommend to use \textbf{-np > 1}. It is not mandatory but I have used a Master-Worker pattern, where RANK 0 is the Master and RANK 1+ are Workers. To execute the program use the following command:
\begin{lstlisting}
    $ cd "project-root"
    $ cd src/
    $ make mpiff
    $ mpirun -np 2 ./mpiff 1M 256 2 32
\end{lstlisting}

\subsubsection{Simulation Scripts}
It is also possible to run all the programs with arbitrary number of inputs and workers. Based on the number of input types I have designed 4 different scripts, which runs 4 different number of record inputs with 2 4 8 16 32 48 64 128 number of workers. The scripts can be run using the following commands:
\begin{lstlisting}
    $ cd "project-root"
    $ cd src/
    $ bash run_1M_256.bin 32
\end{lstlisting}
The last parameter is the \textbf{memory cap}, you can put your desired number to test the behavior. If you are using \textbf{spmcluster.unipi.it}, make sure you are using SRUN commands. To facilitate that you can use the following command:
\begin{lstlisting}
    $ cd "project-root"
    $ cd src/
    $ bash run_simulation.sh 1M_256 32
\end{lstlisting}
Here the parameters are number of records and memory cap. As input records, this script supports the following parameters: 1M\_256, 5M\_128, 10M\_64, 100M\_16, ALL.\\[10pt]
But the MPI+FastFlow version has a different script due to different requirement. To run the simulation of MPI+FastFlow, use the following command:
\begin{lstlisting}
    $ cd "project-root"
    $ cd src/
    $ bash run_mpi.sh 32
\end{lstlisting}

\section{Implementation}
\subsection{Sequential Version}
As per the project description I have leveraged \texttt{std::sort()} function from \texttt{C++}. But to make performance comparison and suitable study of speedup and efficiency, we need a program that is purely sequential. For that I have programmed another version where reading, sorting and merging happens 100\% sequentially. No communication, no overlapping and no overhead.
\subsection{Tunable Constants}
As we have discussed earlier about some Tunable constants which can help running the program in different machines in different scenarios.

\subsubsection{DEGREE}
Is a constant that depends on NUMBER OF WORKERS set to run in the program. Right now the decided value for this is:
\begin{footnotesize}
    \begin{lstlisting}[escapechar=\%]
        DEGREE = % $WORKERS^2$ %
    \end{lstlisting}
\end{footnotesize}
This value is later used to determine the DISTRIBUTION\_CAP. The intention is to decide a value(in bytes) for the emitter to read from the file in each turn and then slice them equally in such a way that one PE can put one slice easily in it's private cache.

\subsubsection{DISTRIBUTION\_CAP}
Is the amount of bytes read from the input file in each turn. Right now the decided value is:
\begin{lstlisting}[escapechar=\%]
    uint64_t l1_cache = sysconf(_SC_LEVEL1_DCACHE_SIZE);
    uint64_t cap = l1_cache % $\times$ % DEGREE;
    DISTRIBUTION_CAP = std::max(8388608UL, cap);
\end{lstlisting}
But a minimum of 8MiB has been set because much lower values creates communication overheads, and also very smaller writes in the \texttt{OOC} mode. But making this value larger will create an I/O delay and during that time workers will be idle. Also, we know that smaller read/writes are faster than big ones in mechanical discs. This constant is therefore tuned based on the current configuration of \texttt{spmcluster}, for this implementation.

\subsubsection{MEMORY\_CAP}
Is the size-limit of MEMORY in GiBs. According to the project requirement, for each node we have a memory limit of 32GiB. But to test the program with a 32GiB input is hard and time consuming.So, instead of generating an input of 32GiB we can use this constant to limit the memory size and test the behavior of our program.

\subsection{Algorithms}
\subsubsection{The Emitter}
\begin{footnotesize}
    \begin{lstlisting}
        1. Check file_size := get_file_size(input_file)
        2. IF file_size <= MEMORY_CAP THEN
            mode := IN_MEMORY
        ELSE
            mode := OUT_OF_CORE
        END IF
        EMITTER():
        3.  segment_id := 0
        4.  WHILE NOT end_of_file DO
                segment := []
                accumulator := 0
                WHILE accumulator < DISTRIBUTION_CAP AND NOT end_of_file DO
                    record := read_record(input_file)
                    record_size := sizeof(key) + sizeof(len) + len
                    IF accumulator + record_size > DISTRIBUTION_CAP AND segment NOT empty THEN
                        BREAK
                    END IF
                    segment.append(record)
                    accumulator := accumulator + record_size
                END WHILE
                ranges := slice_ranges(segment.size(), DEGREE)
                FOR each range [L, R] in ranges DO
                    slice := segment[L:R]
                    task := Task{slice, mode, segment_id, slice_index}
                    ff_send_out(task)
                END FOR
                segment_id := segment_id + 1
            END WHILE
    \end{lstlisting}
\end{footnotesize}

\subsubsection{The Worker}
\begin{footnotesize}
    \begin{lstlisting}
        WHILE TRUE DO
            task := receive_task()
            IF task is EOS THEN
                BREAK
            END IF
            std::sort(task.slice)
            result := TaskResult{task.slice, task.mode, task.segment_id, task.slice_index}
            IF task.mode = IN_MEMORY THEN
                result.kind := InMemBatch
            ELSE
                result.kind := SortedSlice
            END IF
            send_to_collector(result)
        END WHILE
    \end{lstlisting}
\end{footnotesize}

\subsubsection{The Collector}
\begin{footnotesize}
    \begin{lstlisting}
        COLLECTOR():
        1.  segments_map := empty map[segment_id =: slices[]]
        2.  inmem_batches := []
        3.  run_paths := []
        4.  WHILE receiving results DO
            result := receive_from_worker()
            IF result.kind = InMemBatch THEN
                inmem_batches.append(result.slice)
                ELSE IF result.kind = SortedSlice THEN
                    segments_map[result.segment_id].append(result.slice)
                    IF segments_map[result.segment_id].size() = DEGREE THEN
                        run_path := k_way_merge_to_file(segments_map[result.segment_id])
                        run_paths.append(run_path)
                        delete segments_map[result.segment_id]
                    END IF
                END IF
            END WHILE
        5.  IF mode = IN_MEMORY THEN
            k_way_merge_to_file(inmem_batches, output_file)
            ELSE
                FOR each segment_id in segments_map DO
                    IF segments_map[segment_id] NOT empty THEN
                        run_path := k_way_merge_to_file(segments_map[segment_id])
                        run_paths.append(run_path)
                    END IF
                END FOR
                k_way_merge_runs(run_paths, output_file)
                delete_files(run_paths)
            END IF
    \end{lstlisting}
\end{footnotesize}
The core algorithm for all versions is same. Based on the technological stack, a few additions are made but they didn't make any significant change.

\subsection{FastFlow}
The FastFlow implementation uses the farm pattern (building-blocks) with three stages: Emitter, Worker, and Collector. The program first checks if the input file size fits within MEMORY\_CAP to decide between in-memory or out-of-core mode. The Emitter reads the input file and creates segments bounded by DISTRIBUTION\_CAP, then divides each segment into DEGREE cache-sized slices using the slice\_ranges function. These slices are distributed to workers using set\_scheduling\_ondemand(DEGREE), which enables automatic pull-based load balancing where idle workers request tasks from the emitter. FastFlow handles back-pressure automatically through internal bounded SPSC(Single Producer Single Consumer) channels.When all workers are busy and their input queues are full, \texttt{ff\_send\_out} blocks the emitter until workers consume tasks. Workers sort their assigned slices using std::sort and send results to the Collector.\\
In out-of-core mode, the Collector accumulates slices by segment\_id and when DEGREE slices arrive for a segment, it immediately performs a k-way merge using a min-heap and writes the sorted segment(one DISTRIBUTION\_CAP) to a temporary run file, then frees memory. After all segments are processed, incomplete segments are flushed and a final k-way merge of all run files produces the output. In in-memory mode, the Collector accumulates all sorted slices and performs a single k-way merge directly to the output file. \textbf{The implementation achieved similar results like OpenMP, but required significantly less code with no manual SafeQueue implementation, no mutex or condition\_variable management, and no explicit synchronization primitives. The simplified code structure demonstrates that high-level structured parallelism frameworks can deliver competitive performance with substantially reduced implementation complexity. With targeted optimizations such as tuning DEGREE and DISTRIBUTION\_CAP values, experimenting with custom FastFlow schedulers, implementing asynchronous disk I/O, the FastFlow version has strong potential to match or even exceed other framework's performance.}

\subsection{OpenMP}
The OpenMP implementation evolved through multiple versions (openmpv1\~5) before reaching the final optimized version in \textbf{openmp.cpp}. The architecture follows a three-stage farm pattern using \verb|#pragma omp task|: Emitter, Workers and Coordinator. Unlike FastFlow's built-in abstractions, this implementation requires a manual \texttt{SafeQueue<T>} template class with explicit \texttt{std::mutex}, \texttt{std::condition\_variable cv\_consumer, cv\_producer} for thread-safe communication between stages. The Emitter reads segments bounded by DISTRIBUTION\_CAP, slices them into DEGREE cache-sized pieces, and pushes tasks to the task queue which blocks when full through \texttt{cv\_producer.wait()} implementing explicit backpressure. Workers compete for tasks using \texttt{task\_queue.pop()}, sort their slices with \texttt{std::sort}, and push results to the sorted queue. The Coordinator batches complete segments (DEGREE slices) and sends them to a write queue for out-of-core mode, or accumulates all data for in-memory direct writing. A dedicated writer thread pool of WORKERS/2 threads processes write tasks in parallel to overlap sorting with I/O operations (this is completely additional compared to FastFlow. I did nothing like this in FastFlow, yet FastFlow provides better results than OpenMP), requiring an additional \texttt{std::mutex paths\_mutex} with \texttt{std::lock\_guard} to protect the shared \texttt{run\_paths} vector. Termination uses poison pills: the Emitter sends num\_workers poison pills to workers, workers forward them to the Coordinator, and the Coordinator sends num\_writers poison pills to writers. Queue sizes are manually tuned to \texttt{DISTRIBUTION\_CAP * WORKERS} for task and sorted queues, and \texttt{num\_writers} for the write queue to balance buffering and memory consumption. After the pipeline completes, a final k-way merge combines all run files into the output. This implementation achieved similar results like FastFlow, but required substantially more code—over 80 additional lines for SafeQueue implementation, explicit synchronization primitives throughout the farm, careful queue size tuning, and manual coordination of all inter-stage communication compared to FastFlow's declarative approach.

\subsection{MPI+FastFlow}

The MPI+FastFlow hybrid implementation extends the solution to a multi-node systems by combining MPI for inter-node communication with FastFlow for intra-node parallelism. I have used master-worker pattern where the master process (rank 0) reads the input file, creates segments bounded by \texttt{DISTRIBUTION\_CAP}, and dynamically distributes them to idle worker nodes using \texttt{MPI\_Send} with a custom \texttt{MessageHeader} protocol containing segment metadata and item counts. Each worker node (rank 1+) receives a segment via \texttt{MPI\_Recv}, instantiates a complete FastFlow farm (Emitter, Workers, Collector) to process that segment internally, performs k-way merge of the sorted slices, and sends the merged result back to the master using \texttt{MPI\_Send}. The master collects all sorted segments and performs a final k-way merge to produce the output, writing intermediate run files for out-of-core mode or merging directly for in-memory mode. I chose FastFlow over OpenMP for the hybrid version because FastFlow's encapsulated farm pattern is significantly easier to integrate with MPI where each worker node simply instantiates a self-contained farm without managing explicit \texttt{SafeQueue}, \texttt{mutex}, or \texttt{condition\_variable} primitives that would require careful coordination with MPI communication. OpenMP's task-based parallelism with manual synchronization would introduce complexity in managing thread pools alongside MPI message passing, particularly handling back-pressure and termination across both frameworks. FastFlow's automatic internal synchronization through bounded SPSC channels operates independently of MPI, providing clean separation between inter-node (MPI) and intra-node (FastFlow) parallelism. Additionally, FastFlow farms can be created and destroyed per segment without coordination overhead, while OpenMP would require persistent thread pools and more complex state management across MPI operations. This architectural simplicity allows the hybrid implementation to scale horizontally across nodes while maintaining the same efficient intra-node parallelism as the single-node FastFlow version.

\section{Performance Evaluation}
In this section we are going to discuss the performance analysis of our implementations. To achieve different results we have varied number of records with different payload size and workers. The tests are conducted in \texttt{spmcluster.unipi.it} with appropriate \texttt{SRUN} commands. For the tests we are considering 3 different inputs: \texttt{1M\_256, 5M\_128, 10M\_64} and a set of workers: \texttt{2,4,8,16,32,48,64,128}. The goal is to check oversubscription issues, speedup and efficiency of the program between two technologies and single vs multi-node.

\subsection{The Analysis}
The timings are studied in different stages of the implementation. I have calculated individual timings for \texttt{Emitter, Worker and Collector}. The \texttt{Worker's} time is purely computational(sorting). And one final timing has been recorded for the overall execution of the program. So, during the performance analysis we will consider both \texttt{Worker's} and \texttt{Overall} time. We will analyze our program by varying \texttt{Number of Records(N), Payload(P) and Workers(W)}. As we have implemented \texttt{FARM}, the emitter, worker and collector, all of them are considered as FastFlow/OpenMP threads, hence workers. But in the implementation \texttt{Number of Workers(W)}(who performs the sorting) is equal to W. The reason I did that is because, Emitter and Collector are not always busy processing. While testing the implementation, I have noticed that the Collector is idle most of the time(specifically for IN\_MEMORY operation). When the Collector is in action, the emitter and workers are idle. So, basically this is a trade-off situation and we can generate 2 extra threads(Emitter and Collector) to confirm how our program behaves with N number of pure workers.
\subsubsection{Varied N, P and W}
For number of records \texttt{N}, we are considering 1M(million), 5M and 10M with payload size 256, 128 and 64. For the testing I have also put a fixed value of 8MiB as the \texttt{DISTRIBUTION\_CAP}, so that all setup read and process same amount of data.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{OMP_WT.png}
    \caption{OpenMP Working Time}
    \label{fig:omp_wt}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{FF_WT.png}
    \caption{FastFlow Working Time}
    \label{fig:ff_wt}
\end{figure}
\noindent Figures \ref{fig:omp_wt} and \ref{fig:ff_wt} we can clearly state that parallelism is not \texttt{proportional} to number of workers. We started having improved working time as number of workers increases, but after a certain point it starts decreasing again. This explains the issue of \texttt{oversubscription}. In our \texttt{spmcluster}, we have 20 physical cores with 2 threads in each core making it 40 logical cores. The machine also has 640KiB of data cache for each physical core with NUMA aware technology. From the table we can see that the program performs optimally with 32-64 workers depending on the workload, and the performance drastically degrades with 128 workers. This is clearly an oversubscription issue where the number of threads exceeds available logical cores, causing excessive context switching and cache thrashing.The FastFlow implementation shows exact same behavior but it beats the performance of OpenMP by a few milliseconds. Even though OpenMP beats FastFlow by a few milliseconds again in total execution time, with more optimization we can achieve similar or even better results.\\[5pt]
The program behavior also changes based on Number of records and payload size. For each computation, there is a fixed cycle cost which increases with number of records. If we have small payload size, the workers can accumulate more records into its private memory(caches). Processing those records will have more fixed overhead than less items in the private memory(caches). Also processing more items increases possibility of higher cache misses. But it is not universal that with more records we will always get bad performance, it is always a trade-off.

\subsubsection{Speedup and Efficiency}
\texttt{Figure 2} illustrates that the speedup is not \texttt{super-linear} but \texttt{sub-linear}. It is not guaranteed that with more workers we will have better performance. We will achieve the optimal performance with optimal number of workers for each type of input.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Speedup.png}
    \caption{Speedup}
    \label{fig:speedup}
\end{figure}
\noindent By varying number of records and payload size we can see that for different inputs we have optimal speedup with different number of workers. For example: we achieved the highest speedup for 1M records of 256 bytes payload with 64 workers in OpenMP. Also with FastFlow we achieved highest speedup for 1M\_256 with 64 workers. But for 5M records with 128 bytes payload the result changes significantly. We achieved highest speedup with 64 workers in OpenMP and only with 16 workers in FastFlow. The same is for 10 million records with 64 bytes payload, we achieved highest speedup with 32 workers in OpenMP and with 16 workers in FastFlow. This means FastFlow is more efficient for our implementation.\\[10pt]
For the efficiency [\texttt{Figure 3}] we see a continuously degrading curve. This clearly demonstrates that achieving speedup with $N$ workers does not guarantee efficient resource utilization. We may be paying a significant cost in additional workers for marginal performance improvements. The critical question becomes: what is the optimal parallelism degree $\rho$ that balances speedup and efficiency? There is no universal answer, as the choice depends on optimization criteria and constraints. We select $\rho$ based on the scenario:
\begin{itemize}
    \item \textbf{Time-critical applications:} When deadline constraints
          dominate, choose $\rho$ maximizing speedup regardless of efficiency
          (e.g., 64 workers for 1.80× speedup at 2.8\% efficiency).

    \item \textbf{Resource-constrained scenarios:} When computational
          resources are limited or expensive, choose $\rho$ where efficiency
          remains acceptable (typically >50\%, or before the "knee" of the
          speedup curve where diminishing returns emerge). For our workload,
          this occurs around 8-16 workers.

    \item \textbf{Balanced approach:} Evaluate the marginal cost—if
          doubling workers yields <20\% speedup improvement, the additional
          resources are poorly utilized and a lower $\rho$ is preferable.
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Efficiency.png}
    \caption{Efficiency}
    \label{fig:efficiency}
\end{figure}

\subsubsection{Strong vs Weak Scalability}
For the weak and strong scalability we will consider $T_{service}^{farm}$ time for all executions. Till now we had our discussion on \texttt{Execution Time}(sorting-time) only to have a clear picture of workers' performance. But to discuss scalability we have to take \texttt{Emitter} and \texttt{Collector's} time into consideration as well. The service time also reflects a similar scenario where we can see the sub-linear behavior. The service time of \texttt{FARM} has been calculated by the this equation:
\[T_{service}^{farm}=max\{\frac{T_{s}^{w}}{K}, T_{s}^{e}, T_{s}^{c}\}\]
From the equation and table\texttt{[Figure 5]} data we can say that the \texttt{Collector Stage}(detailed in bottleneck discussion) dominates the performance, but there is a difference in varied nodes. According to \texttt{Strong Scalability} we should achieve reduced service time(super-linear speedup) on a fixed problem size with increasing number of \texttt{PEs}. Let us fix our test input as 10 million records with 64 bytes long payload\texttt{(10M\_64)} with 8 workers. We can see the execution time is decreasing as we include more resources\texttt{(NODES)}. We start with 2 Nodes and 8 workers with a service-time of \texttt{107 seconds}, and gradually with \texttt{4, 6, 8} nodes the time reduces. But from $6^{th}$ node it starts increasing again. Which is not \texttt{super-linear} but \texttt{sub-linear}[Figure 6], and proves \texttt{Amdahl's Law} stating that
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{service_time_ff.png}
    \caption{Service Time MPI+FastFlow(FARM)}
    \label{fig:service_time_ff}
\end{figure}
\noindent
performance will always be limited by the sequential part of the code. Since our program has dependencies on different memory levels like\texttt{Storage Devices, DRAMs, Caches} and \texttt{Concurrent Access on Data}, we can only scale a portion of the code to achieve optimal performance.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{strong_scalability_curve.png}
    \caption{Strong Scalability Curve - MPI+FastFlow}
    \label{fig:strong_scalability}
\end{figure}
\noindent
If we consider $f$ as the sequential part of the code, we can derive the equation from \texttt{Amdahl's Law}:
\[Speedup, S(\rho)=\frac{T_{c}(1)}{T_{c}(\rho)}=\frac{1}{f+\frac{1-f}{\rho}}\]
And for our program $f$ is dominating, and this is why we achieve very little performance boost. The scenario is also similar for weak scalability \texttt{(Gustafson's Law)}. The law says if we scale the problem size along with the resources we can find more portion of the code to be benefitted by the parallelism. Let us consider $\alpha$ as the sequential part and $\beta$ benefitted part from the parallelism. We still have $f$ as the part which can be benefitted from the parallelism, and thus we can form this equation form the \texttt{Gustafson's law}:
\[S_{\gamma}(\rho)=\frac{\alpha f+\beta(1-f)}{\alpha f+\frac{\beta (1-f)}{\rho}}\]
\newpage
\noindent
From the table we can see the following behavior:
\begin{itemize}
    \item \textbf{2 nodes:} 1M $\rightarrow$ 11.35s, 5M $\rightarrow$ 52.51s, 10M $\rightarrow$ 96s
    \item \textbf{4 nodes:} 1M $\rightarrow$ 8.8s, 5M $\rightarrow$ 37.41s, 10M $\rightarrow$ 70s
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{weak_scalability.png}
    \caption{Weak Scalability Curve - MPI+FastFlow}
    \label{fig:weak_scalability}
\end{figure}

\section{Final Analysis}
In this section I am going to discuss about the \texttt{cost-model} of the distributed solution I have proposed. Different time studies will be considered for the cost-model. To keep things simple I am not attaching the logs from the code, but they can be enabled in the \texttt{farm.cpp} codebase and studied.
\subsection{Cost Model}
Lets denote the parameters for our cost-model first \texttt{[for 1M records with 256 bytes payload and 1 worker]}:

\begin{flushleft}
    $\displaystyle \text{Inter-arrival Time, } =T_{arrival}= 29 [ms]$\\[5pt]
    $\displaystyle \text{Service time of a Worker, } =T_{s} ^w=7 [ms]$\\[5pt]
    $\displaystyle \text{Emitter Total, } =T_{s} ^e= 491 [ms]$\\[5pt]
    $\displaystyle \text{Collector Total, } =T_{s} ^c= 951 [ms]$\\[5pt]
    $\displaystyle \text{Number of Optimal Workers, } K_{opt}=\frac{T_{s} ^w}{T_{arrival}}=\frac{29}{7}=4.1 \approx 4$\\[5pt]
    $\displaystyle T_{service}^{farm}=max\{\frac{T_{s}^{w}}{K},T_{s}^{e},T_{s}^{c}\}=max\{4,491,951\}=951[ms]$
\end{flushleft}
\noindent
This is the \texttt{Service-Time} of our \texttt{FARM} implementation using FastFlow. Note that this service-time ignores the communication overhead but include communication over computation overlapping. We are emitting each segment equal to \texttt{DISTRIBUTION\_CAP} bytes. From the second segment we are overlapping the communication between emitter and workers. But the significant part is the final merge, where we are doing k-way merge but still writing one big \texttt{<.bin>} file. That is the largest sequential part of our code which dominates everything(\texttt{strong-scaling by Amdahl's law}). But the program gives satisfactory results in the \texttt{MEMORY-OOC} mode where we are writing each small segments before the final merge. At that mode we achieve 2 ways \texttt{computation-to-communication} overlapping, because from the second segment we start overlapping emitting and sorting and from the third segment we achieve emitting-sorting-writing. And using that mode we can sort \texttt{TeraBytes}. Even though in this report I have discussed mostly the \texttt{IN\_MEMORY} operation but in the implementation I have focused more on the \texttt{OOC} mode. The report pages are limited that's why I couldn't bring it here but I would request you to visit the github page if anyone is interested. Here is an image from a test of 100M records with 32 bytes long payload using FastFlow(farm).
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{ooc.png}
    \caption{Memory Out of Core operation on 3.6GiB file with 1GiB Memory}
    \label{fig:ooc}
\end{figure}
\noindent
\texttt{Figure [8]} demonstrates that we can sort large files in \texttt{OOC} mode without paralyzing the whole system. Even though the memory cap is 1GiB, the program is consuming more than 1GiB. It is very common because there are overheads through out the processing. A breif study is given under \texttt{/src/discussions/} with test codes to prove the point.
\subsection{The Bottleneck Phase}
This is the most important part of the whole report. By now we must have become hopeful to see a result that will amaze us! But it is not going to do so. Two main laws of parallel paradigm \texttt{Amdahl's and Gustafson's } law didn't just show the handicap of parallelism they actually broke the dilemma. That is why years after years computer scientists are still doing research on it. Parallelism is not just about \texttt{PEs} with higher frequency clock, it is a combination of the whole eco system. And in my program I can see that very clearly. Lets see two output of a run with 1M records of 256 bytes payload and 2, 4 and 8 workers.
\begin{itemize}
    \footnotesize
    \item M: IN-MEMORY | R: 1M | PS: 256 | W: 2 | DC:8MiB | ET: 456.331 ms | WT: 98.008 ms | CT: 767.576 ms | TT: 1.277 s
    \item M: IN-MEMORY | R: 1M | PS: 256 | W: 4 | DC:8MiB | ET: 901.708 ms | WT: 96.079 ms | CT: 988.654 ms | TT: 1.944 s
    \item M: IN-MEMORY | R: 1M | PS: 256 | W: 8 | DC:8MiB | ET: 1.484 s | WT: 113.282 ms | CT: 874.681 ms | TT: 2.436 s
\end{itemize}
\noindent
What do we see from the outputs? We can see that a significant cost is being paid at emitter and collector stage. While the workers are sorting the data much faster, emitter cannot read that fast from the disc and collector cannot write on the disc. Mechanical discs has limitations, even though these days we have SSDs and NVMEs but their speed is nothing comparable to the CPU. Also the cache-memory should be taken under consideration because CPUs recursively fetch data from the memory and operates. So, this is a minimum 3 way operation(ignoring other memory hierarchy): Disc $\rightarrow$ Memory $\rightarrow$ Cache. So, we can declare that for us in this problem \texttt{EMITTER \& COLLECTOR} are the bottleneck phases. Now the question is, how can we get rid of it? Does this mean in all parallel programs we will have these same issues? The answer is \textbf{NO!}. Adopted solution always depends on the type of problem we have. \textbf{In this program we are not doing anything that is computationally heavy. We are reading binary bits from the file, forming items, sending to the workers and the workers are just performing CAS(compare \& swap). Also, we introduced a technique for the workers so that they can sort everything privately. Also there is no data dependencies like reusing generated results from other workers. That is why workers became the fastest throughout the whole operation.} So, we could have simply parallelized the \texttt{EMITTER \& COLLECTOR}, isn't it? Yes! That could be one solution which I have also tried but it introduces another problem. \texttt{Race-Condition} is another significant term in parallelism. The goal of our program is to sort the data, which means the order must be maintained. Now, the workers are sorting the data privately without knowing which slices of the input are sorted by other workers. The final \texttt{k-way} merge is performed by the collector who ensures the sorted manner of the records. Imagine having multiple collectors trying to write the records in the same binary file. \textbf{It is common that they will write intermediate bits which will not only disturb the sorting nature but also corrupting the information. To solve that problem we can introduce locks but that doesn't solve the problem because multiple threads of the collector will fight to achieve that lock which will generate more communication overheads.} So, I will say that this solution is not 100\% optimal and has so many opportunities for improvement in future. But the \texttt{out-of-core} mode is really helpful for machines with limited resources. \textbf{Also for shared hosting machines where we have multiple users accessing the database frequently}, the ooc mode is really helpful for slicing big files into smaller ones for better efficiency. That is all, but lets discuss some other problems which were also crucial for the better performance.
\subsection{Problems \& Adopted Solutions}
During the development I have faced several problems regarding data-structure, load-distribution, synchronization and I/O handling. Each of these problems have been solved based on the theoretical part we are taught during the course. Even though I cannot say I have solved 100\% of them, but I can assure you this is a good starting point. From this point it can be more optimized. Lets discuss the problems and solutions.
\subsubsection{Data Structure}
In the initial implementation I used \texttt{std::vector<uint8\_t>} for storing variable-length payloads, which created a critical memory consumption issue. The structure \texttt{std::vector<Item>} contained items where each Item had a \texttt{std::vector<> payload}, resulting in nested vectors. The fundamental problem is that \texttt{std::vector<uint8\_t>} consumes 24 bytes of overhead per instance to store internal pointers (begin, end, capacity) and bookkeeping information, while a simple \texttt{uint8\_t*} pointer consumes only 8 bytes. For large datasets, this overhead becomes catastrophic.
    {
        \renewcommand{\thefootnote}{**}Experimental measurements\footnote{\textbf{the experimental program can be found under /src/discussion/tests}}
    }
show that 100 million records using \texttt{std::vector} consumed 2288 MB of memory compared to only 762 MB using the \texttt{CompactPayload} pointer-based approach, a difference of 1526 MB (66\% overhead). This represents a waste of 16 bytes per record purely for vector metadata. For the 100M dataset, this overhead alone required an additional 1.5 GB of memory before storing any actual payload data. The \texttt{CompactPayload} class was designed to replace the vector by using a single \texttt{uint8\_t*} pointer that points to a contiguous memory block containing the payload size (uint32\_t) followed by the actual data bytes. This design maintains the flexibility of variable-length payloads while reducing per-record overhead from 24 bytes to 8 bytes. The 16-byte savings per record prevented memory exhaustion and system crashes during sorting operations on large datasets, demonstrating that careful data structure selection is critical for memory-bounded external sorting algorithms.

\subsubsection{OpenMP}
\begin{enumerate}
    \item \textbf{Sequential Bottleneck Due to No Data Distribution:} The first version has a critical design flaw where the emitter reads the entire \texttt{MEMORY\_CAP} amount of data before workers could start processing. This meant parallelism only occurred during the sorting of slices, not during data reading. Workers remained completely idle while the emitter was reading, which defeated the purpose of parallel processing(computation overlapping communication). This sequential bottleneck was particularly severe for large datasets, as the emitter had to read gigabytes of data before any computation could begin. \\[5pt] \textbf{Solution:} In version 3, we introduced the \texttt{DISTRIBUTION\_CAP} parameter to limit how much data the emitter reads at once, forcing data distribution in smaller chunks and enabling workers to start processing while more data is being read.

    \item \textbf{Memory Paralyzed and System Crashes:} Testing revealed a shocking memory consumption issue: a 2.7 GiB input file consumed 5.9 GiB of memory after reading. The root causes were struct overhead (16 bytes per Item), memory allocation metadata (8-12 bytes), and vector capacity growth (allocating in powers of 2). For 100 million records with 128-byte payloads, this overhead could exhaust system memory and crash the machine. Additionally, vectors were not freed until program completion, and the emitter would start reading the next segment before workers finished processing the current one, compounding the memory pressure. \\[5pt] \textbf{Solution:} Version 2 introduced memory cleanup by freeing memory after each intermediate write, preventing system paralysis. Version 3 further mitigated this by using \texttt{DISTRIBUTION\_CAP} to limit simultaneous memory allocation. The vector overhead itself was later addressed with the \texttt{CompactPayload} data structure.

    \item \textbf{No Pipeline Parallelism:} In the out-of-core scenario, after the emitter read \texttt{MEMORY\_CAP} worth of data and workers sorted it, the collector would perform the k-way merge and write the segment while the emitter and all workers sat completely idle. For processing 20 GiB of data with a 2 GiB \texttt{MEMORY\_CAP}, this meant 10 sequential cycles of read-sort-merge-write with no overlap, resulting in disastrous performance. The stages were not pipelined, so only one stage (reading, sorting, or merging) was active at any time. \\[5pt] \textbf{Solution:} Version 3 achieved partial pipeline parallelism by introducing \texttt{DISTRIBUTION\_CAP}, though the emitter would pause during processing and workers would pause during the next read. Version 4 improved this significantly by introducing queues for unsorted and sorted tasks, allowing the emitter, workers, and collector to operate more concurrently.

    \item \textbf{ Static Scheduling and Load Imbalance:} Even with \texttt{DISTRIBUTION\_CAP} in version 3, workers became idle because the number of tasks equaled the number of workers, and the scheduling pragma (schedule(dynamic)) was ineffective. When workers finished their slices at different times due to variable payload sizes, faster workers had nothing to do while slower workers were still processing. This under-utilization was exacerbated by the emitter pausing while workers processed, creating alternating periods of reading and computing instead of continuous overlap. \\[5pt] \textbf{Solution:} Version 4 introduced task queues with mutex locks and generated \texttt{workers * n} tasks instead of just tasks equal to workers, providing more fine-grained work distribution. This kept workers busy with smaller tasks and improved load balancing across threads.

    \item \textbf{Unbounded Queue Growth and Memory Pressure:} Version 4 introduced queues without size limit. The sorted task queue would grow unboundedly because the collector was slower at merging and writing than workers were at sorting. Memory was not released immediately after sorting, causing accumulation. Additionally, writing intermediate files of \texttt{MEMORY\_CAP} size was slow, and during these slow writes, the emitter continued reading and workers continued sorting, filling the queues endlessly. This led to IO contention and risked memory exhaustion despite solving earlier distribution issues. \\[5pt] \textbf{Solution:} Version 5 introduced bounded queues with explicit queue size limits, implementing back-pressure that would pause the emitter when queues filled up. This prevented unbounded memory growth and ensured the system operated within memory constraints, especially beneficial for \textbf{out-of-core operations}.

    \item \textbf{Collector Bottleneck and Suboptimal Cache Utilization:} Later I found the collector as the bottleneck, unable to keep up with the rate at which workers produced sorted slices. Additionally, earlier versions did not consider cache hierarchy when sizing work units. Slices were not optimized for L1 cache size, meaning workers performed sorts on data larger than their fastest cache level, incurring memory access penalties. The intermediate write strategy also needed optimization, as a single collector writing sequentially couldn't match the throughput of multiple parallel workers. \\[5pt] \textbf{Solution:} The final version introduced multiple optimizations: \texttt{DEGREE} parameter to create slices that fit in L1 cache (improving sort performance), \texttt{DISTRIBUTION\_CAP} sized as \texttt{L1\_CACHE * DEGREE} to balance memory usage and cache efficiency, and \texttt{WRITERS = WORKERS/2} threads dedicated to parallel intermediate writes from the sorted task queue. This allowed the emitter and workers to continue operating while a pool of writer threads handled I/O, eliminating the collector bottleneck.
\end{enumerate}

\subsubsection{FastFlow}
In FastFlow I didn't face these much issues because most of the functionalities were taken care of by FastFlow. The memory overflow and no-pipeline parallelism issues were there but when I fixed them for OpenMP, it also got fixed for FastFlow. Because that was the problem of data-parallelism not of FastFlow. The automated scheduling of FastFlow made things much easier. \texttt{And that is also the reason why I preferred FastFlow over OpenMP for the MPI implementation}. MPI itself introduces communication overheads, and to handle things manually with openmp would make it more difficult.

\section{Conclusion}
I would like to say some final words before concluding. Even though it is a student project but still this project covers almost every concept of parallel and distributed systems. It is a good starting point to master someone's skill in parallel programming. We got introduced with technologies like FastFlow, OpenMP and MPI through this project. If we combine our theoretical knowledge with these technologies we can design efficient programs to solve serious problems. Even in my thesis I have planned to use the same concepts for path computation and spectrum allocation. I have said that earlier and I am going to say it again, this project is not 100\% optimized but it is a good starting point. It was my course final project but it has topics worthy enough of having a thesis work. Points like data distribution, synchronization with cache levels, sequential or parallel access to single entity and optimization for legacy systems can bring new ideas. Finally, I would like to thank our professor for introducing FastFlow. The framework eases the work for a programmer by handling so many things automatically. By studying the framework more, we can come up with different solutions of different problems.

\begin{thebibliography}{9}
    \bibitem{textbook}
    Course Contents of Parallel and Distributed Systems: Paradigms and Models [2024/2025]
    \bibitem{href}
    FastFlow Parallel Framework [http://calvados.di.unipi.it/][https://github.com/fastflow/fastflow]
    \bibitem{href}
    OpenMP [https://www.openmp.org/resources/openmp-compilers-tools/\#compilers]
    \bibitem{href}
    MPI [https://www.open-mpi.org/]
\end{thebibliography}
\end{document}
