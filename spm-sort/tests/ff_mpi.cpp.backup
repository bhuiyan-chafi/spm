#include "main.hpp"
#include <mpi.h>

/**
 * MPI + FastFlow Hybrid External Sort
 * 
 * Architecture:
 * - MPI: Multi-node distribution (master distributes segments to worker nodes)
 * - FastFlow: Intra-node parallelism (each worker node runs a FastFlow farm)
 * 
 * Master (rank 0):
 * - Reads input file, creates segments at DISTRIBUTION_CAP boundaries
 * - Dynamically distributes segments to idle worker nodes via MPI
 * - Collects sorted segments from workers
 * - Performs final k-way merge
 * 
 * Workers (rank 1+):
 * - Receive segment via MPI
 * - Run FastFlow farm: Emitter slices → Workers sort → Collector merges
 * - Send sorted segment back to master via MPI
 */

namespace
{
    using Items = std::vector<Item>;

    // MPI Tags
    constexpr int TAG_WORK = 1;
    constexpr int TAG_RESULT = 2;
    constexpr int TAG_TERMINATE = 3;
    constexpr int TAG_TIMING = 4;

    // ==================== MERGE FUNCTIONS ====================

    struct HeapItem
    {
        size_t slice_idx, item_idx;
        uint64_t key;
        bool operator>(const HeapItem &other) const { return key > other.key; }
    };

    void merge_slices_to_file(std::vector<TaskPackage> &slices, const std::string &output_path)
    {
        std::ofstream out(output_path, std::ios::binary);
        if (!out)
            throw std::runtime_error("Cannot open: " + output_path);

        std::priority_queue<HeapItem, std::vector<HeapItem>, std::greater<HeapItem>> heap;

        // Initialize heap with first item from each slice
        for (size_t s = 0; s < slices.size(); ++s)
            if (!slices[s].items.empty())
                heap.push(HeapItem{s, 0, slices[s].items[0].key});

        size_t written = 0;
        while (!heap.empty())
        {
            auto current = heap.top();
            heap.pop();
            const Item &item = slices[current.slice_idx].items[current.item_idx];
            write_record(out, item.key, item.payload);
            ++written;

            const size_t next_idx = current.item_idx + 1;
            if (next_idx < slices[current.slice_idx].items.size())
                heap.push(HeapItem{current.slice_idx, next_idx,
                                   slices[current.slice_idx].items[next_idx].key});
        }

        // Free memory after writing
        for (auto &slice : slices)
        {
            slice.items.clear();
            slice.items.shrink_to_fit();
        }
    }

    enum class MsgTag : int
    {
        TaskHeader = 1,
        TaskKey = 2,
        TaskLen = 3,
        TaskPayload = 4,
        ResultHeader = 5,
        ResultKey = 6,
        ResultLen = 7,
        ResultPayload = 8
    };

    struct MessageHeader
    {
        uint8_t spill;
        uint64_t segment_id;
        uint64_t slice_index;
        uint64_t item_count;
    };

    std::vector<std::pair<size_t, size_t>> make_slice_ranges(size_t n, size_t parts)
    {
        if (parts == 0)
            parts = 1;
        std::vector<std::pair<size_t, size_t>> ranges;
        ranges.reserve(parts);
        for (size_t i = 0; i < parts; ++i)
        {
            size_t L = (i * n) / parts;
            size_t R = ((i + 1) * n) / parts;
            if (L < R)
                ranges.emplace_back(L, R);
        }
        if (ranges.empty())
            ranges.emplace_back(0, n);
        return ranges;
    }

    struct TaskPackage
    {
        bool spill{false};
        uint64_t segment_id{0};
        uint64_t slice_index{0};
        Items items;
    };

    struct PendingRecord
    {
        Item item;
        uint64_t bytes{0};
    };

    class TaskGenerator
    {
    public:
        TaskGenerator(std::size_t worker_count, bool &saw_spill)
            : in(DATA_INPUT, std::ios::binary),
              worker_count(worker_count ? worker_count : 1)
        {
            if (!in)
                throw std::runtime_error("Emitter: cannot open input stream");
            namespace fs = std::filesystem;
            std::error_code ec;
            auto size = fs::file_size(DATA_INPUT, ec);
            in_memory_mode = (!ec) && size <= MEMORY_CAP;
            // spdlog::info("[MPI] TaskGenerator initialized: size={} bytes, memory_cap={} bytes, in_memory_mode={}",
            //              size,
            //              MEMORY_CAP,
            //              in_memory_mode);
            saw_spill = !in_memory_mode;
        }

        bool next(TaskPackage &task)
        {
            if (in_memory_mode)
            {
                return next_in_memory(task);
            }
            return next_out_of_core(task);
        }

    private:
        bool next_in_memory(TaskPackage &task)
        {
            Items batch;
            batch.reserve(INMEM_BATCH_RECORDS);
            while (batch.size() < INMEM_BATCH_RECORDS)
            {
                uint64_t key;
                CompactPayload payload;
                if (!read_record(in, key, payload))
                    break;
                batch.push_back(Item{key, std::move(payload)});
            }
            if (batch.empty())
                return false;

            task.spill = false;
            task.segment_id = 0;
            task.slice_index = inmem_batch_id++;
            // spdlog::info("[MPI][TaskGen] In-memory batch {} prepared with {} items", task.slice_index, batch.size());
            task.items = std::move(batch);
            return true;
        }

        bool next_out_of_core(TaskPackage &task)
        {
            while (true)
            {
                if (!current_segment || next_range_index >= current_ranges.size())
                {
                    if (!load_next_segment())
                        return false;
                }

                auto [L, R] = current_ranges[next_range_index++];
                Items slice;
                slice.reserve(R - L);
                for (size_t j = L; j < R; ++j)
                    slice.push_back(std::move((*current_segment)[j]));

                task.spill = true;
                task.segment_id = current_segment_id;
                task.slice_index = next_range_index - 1;
                // spdlog::info("[MPI][TaskGen] Segment {} slice {} prepared with {} items ({}..{})",
                //              current_segment_id,
                //              task.slice_index,
                //              slice.size(),
                //              L,
                //              R);
                task.items = std::move(slice);

                if (next_range_index >= current_ranges.size())
                    current_segment.reset();

                return true;
            }
        }

        bool load_next_segment()
        {
            current_segment = std::make_unique<Items>();
            current_segment->reserve(64'000);
            uint64_t accumulator = 0ULL;

            if (carry)
            {
                accumulator += carry->bytes;
                current_segment->push_back(std::move(carry->item));
                carry.reset();
            }

            while (true)
            {
                uint64_t key;
                CompactPayload payload;
                if (!read_record(in, key, payload))
                    break;
                uint64_t record_size = sizeof(uint64_t) + sizeof(uint32_t) + payload.size();
                if (!current_segment->empty() && accumulator + record_size > DISTRIBUTION_CAP)
                {
                    carry = PendingRecord{Item{key, std::move(payload)}, record_size};
                    break;
                }
                accumulator += record_size;
                current_segment->push_back(Item{key, std::move(payload)});
            }

            if (current_segment->empty())
            {
                current_segment.reset();
                return false;
            }

            current_ranges = make_slice_ranges(current_segment->size(), DEGREE);
            next_range_index = 0;
            current_segment_id = segment_id_counter++;
            // spdlog::info("[MPI][TaskGen] Segment {} loaded: {} items, {} slices",
            //              current_segment_id, current_segment->size(), current_ranges.size());
            return true;
        }

        std::ifstream in;
        const std::size_t worker_count;
        bool in_memory_mode{false};
        uint64_t inmem_batch_id{0};
        std::optional<PendingRecord> carry;
        std::unique_ptr<Items> current_segment;
        std::vector<std::pair<size_t, size_t>> current_ranges;
        size_t next_range_index{0};
        uint64_t current_segment_id{0};
        uint64_t segment_id_counter{0};
    };

    void send_header(int dest, const MessageHeader &header, MsgTag tag, MPI_Comm comm)
    {
        MPI_Send(const_cast<MessageHeader *>(&header),
                 sizeof(header),
                 MPI_BYTE,
                 dest,
                 static_cast<int>(tag),
                 comm);
    }

    MessageHeader receive_header(int source, MsgTag tag, MPI_Comm comm)
    {
        MessageHeader header{};
        MPI_Recv(&header,
                 sizeof(header),
                 MPI_BYTE,
                 source,
                 static_cast<int>(tag),
                 comm,
                 MPI_STATUS_IGNORE);
        return header;
    }

    void send_items(int dest, const Items &items, MsgTag key_tag, MsgTag len_tag, MsgTag payload_tag, MPI_Comm comm)
    {
        for (const auto &it : items)
        {
            MPI_Send(const_cast<uint64_t *>(&it.key), 1, MPI_UINT64_T, dest, static_cast<int>(key_tag), comm);
            uint32_t payload_len = static_cast<uint32_t>(it.payload.size());
            MPI_Send(&payload_len, 1, MPI_UINT32_T, dest, static_cast<int>(len_tag), comm);
            if (payload_len > 0)
            {
                MPI_Send(const_cast<uint8_t *>(it.payload.data()),
                         static_cast<int>(payload_len),
                         MPI_BYTE,
                         dest,
                         static_cast<int>(payload_tag),
                         comm);
            }
        }
    }

    void receive_items(int source,
                       Items &items,
                       std::size_t count,
                       MsgTag key_tag,
                       MsgTag len_tag,
                       MsgTag payload_tag,
                       MPI_Comm comm)
    {
        items.clear();
        items.reserve(count);
        for (std::size_t idx = 0; idx < count; ++idx)
        {
            uint64_t key = 0;
            MPI_Recv(&key, 1, MPI_UINT64_T, source, static_cast<int>(key_tag), comm, MPI_STATUS_IGNORE);
            uint32_t payload_len = 0;
            MPI_Recv(&payload_len, 1, MPI_UINT32_T, source, static_cast<int>(len_tag), comm, MPI_STATUS_IGNORE);
            CompactPayload payload;
            if (payload_len > 0)
            {
                payload.resize(payload_len);
                MPI_Recv(payload.data(),
                         static_cast<int>(payload_len),
                         MPI_BYTE,
                         source,
                         static_cast<int>(payload_tag),
                         comm,
                         MPI_STATUS_IGNORE);
            }
            items.push_back(Item{key, std::move(payload)});
        }
    }

    void send_task(int dest, const TaskPackage &task, MPI_Comm comm)
    {
        MessageHeader header{};
        header.spill = task.spill ? 1 : 0;
        header.segment_id = task.segment_id;
        header.slice_index = task.slice_index;
        header.item_count = task.items.size();
        send_header(dest, header, MsgTag::TaskHeader, comm);
        send_items(dest, task.items, MsgTag::TaskKey, MsgTag::TaskLen, MsgTag::TaskPayload, comm);
    }

    bool receive_task(int source, TaskPackage &task, MPI_Comm comm)
    {
        MessageHeader header = receive_header(source, MsgTag::TaskHeader, comm);
        if (header.spill == TERMINATE_FLAG && header.item_count == 0)
            return false;
        Items items;
        receive_items(source,
                      items,
                      header.item_count,
                      MsgTag::TaskKey,
                      MsgTag::TaskLen,
                      MsgTag::TaskPayload,
                      comm);
        task.spill = header.spill != 0;
        task.segment_id = header.segment_id;
        task.slice_index = header.slice_index;
        task.items = std::move(items);
        return true;
    }

    void send_result(int dest, const TaskPackage &result, MPI_Comm comm)
    {
        MessageHeader header{};
        header.spill = result.spill ? 1 : 0;
        header.segment_id = result.segment_id;
        header.slice_index = result.slice_index;
        header.item_count = result.items.size();
        send_header(dest, header, MsgTag::ResultHeader, comm);
        send_items(dest, result.items, MsgTag::ResultKey, MsgTag::ResultLen, MsgTag::ResultPayload, comm);
    }

    TaskPackage receive_result(MPI_Comm comm, int &source_rank)
    {
        MPI_Status status;
        MessageHeader header{};
        MPI_Recv(&header,
                 sizeof(header),
                 MPI_BYTE,
                 MPI_ANY_SOURCE,
                 static_cast<int>(MsgTag::ResultHeader),
                 comm,
                 &status);
        source_rank = status.MPI_SOURCE;

        Items items;
        receive_items(source_rank,
                      items,
                      header.item_count,
                      MsgTag::ResultKey,
                      MsgTag::ResultLen,
                      MsgTag::ResultPayload,
                      comm);

        TaskPackage result;
        result.spill = header.spill != 0;
        result.segment_id = header.segment_id;
        result.slice_index = header.slice_index;
        result.items = std::move(items);
        // spdlog::info("[MPI][Master] Received result from rank {} -> segment {}, slice {}, spill={}, items={}",
        //              source_rank,
        //              result.segment_id,
        //              result.slice_index,
        //              result.spill,
        //              result.items.size());
        return result;
    }

    void send_terminate(int dest, MPI_Comm comm)
    {
        MessageHeader header{};
        header.spill = TERMINATE_FLAG;
        header.item_count = 0;
        send_header(dest, header, MsgTag::TaskHeader, comm);
    }

    void finalize_in_memory(std::vector<std::unique_ptr<Items>> &batches)
    {
        std::ofstream out(DATA_OUTPUT, std::ios::binary);
        if (!out)
            throw std::runtime_error("Collector(inmem): cannot open output");

        struct HeapNode
        {
            std::size_t batch_index;
            std::size_t item_index;
            uint64_t key;
        };

        struct Compare
        {
            bool operator()(const HeapNode &a, const HeapNode &b) const
            {
                return a.key > b.key;
            }
        };

        std::priority_queue<HeapNode, std::vector<HeapNode>, Compare> heap;
        for (std::size_t b = 0; b < batches.size(); ++b)
        {
            if (!batches[b]->empty())
            {
                heap.push(HeapNode{b, 0, (*batches[b])[0].key});
            }
        }

        std::size_t written = 0;
        while (!heap.empty())
        {
            auto node = heap.top();
            heap.pop();
            const Item &item = (*batches[node.batch_index])[node.item_index];
            write_record(out, item.key, item.payload);
            ++written;
            std::size_t next_index = node.item_index + 1;
            if (next_index < batches[node.batch_index]->size())
            {
                heap.push(HeapNode{node.batch_index, next_index, (*batches[node.batch_index])[next_index].key});
            }
        }
        spdlog::info("Collector(inmem): wrote {} records -> {}", written, DATA_OUTPUT);
    }

    void finalize_spill(const std::vector<std::string> &run_paths)
    {
        if (run_paths.empty())
        {
            spdlog::warn("Collector(ooc): no runs produced");
            std::ofstream(DATA_OUTPUT, std::ios::binary);
            return;
        }

        std::ofstream out(DATA_OUTPUT, std::ios::binary);
        if (!out)
            throw std::runtime_error("Collector(ooc): cannot open output");

        std::vector<std::unique_ptr<TempReader>> readers;
        readers.reserve(run_paths.size());
        for (const auto &path : run_paths)
            readers.push_back(std::make_unique<TempReader>(path));

        struct HeapNode
        {
            uint64_t key;
            std::size_t temp_run_index;
        };
        struct Compare
        {
            bool operator()(const HeapNode &a, const HeapNode &b) const
            {
                return a.key > b.key;
            }
        };

        std::priority_queue<HeapNode, std::vector<HeapNode>, Compare> heap;
        for (std::size_t i = 0; i < readers.size(); ++i)
        {
            if (!readers[i]->eof)
                heap.push(HeapNode{readers[i]->key, i});
        }

        std::size_t written = 0;
        while (!heap.empty())
        {
            auto top = heap.top();
            heap.pop();
            auto &reader = *readers[top.temp_run_index];
            write_record(out, reader.key, reader.payload);
            ++written;
            reader.advance();
            if (!reader.eof)
                heap.push(HeapNode{reader.key, top.temp_run_index});
        }

        spdlog::info("Collector(ooc): wrote {} records -> {}", written, DATA_OUTPUT);

        for (const auto &path : run_paths)
        {
            std::error_code ec;
            std::filesystem::remove(path, ec);
        }
    }

    void master_main(int world_size, TimerClass &total_worker_time)
    {
        if (world_size < 2)
            throw std::runtime_error("MPI farm requires at least 2 processes");

        std::filesystem::create_directories(DATA_TMP_DIR);

        const std::size_t worker_count = world_size - 1; // Number of MPI workers
        bool saw_spill = false;
        TaskGenerator generator(worker_count, saw_spill);

        std::queue<int> idle_workers;
        for (int rank = 1; rank < world_size; ++rank)
            idle_workers.push(rank);

        std::vector<std::unique_ptr<Items>> inmem_batches;
        std::vector<std::string> run_paths;
        uint64_t run_id = 0;

        // Coordinator: batch DEGREE slices per segment before writing (OOC mode)
        struct SegmentInfo
        {
            std::vector<TaskPackage> slices;
            size_t expected_slices = DEGREE;
        };
        std::map<uint64_t, SegmentInfo> segments; // segment_id -> batched slices

        uint64_t tasks_outstanding = 0;
        bool no_more_tasks = false;

        // spdlog::info("[MPI][Master] Starting with {} workers, DEGREE={}, DISTRIBUTION_CAP={}MiB",
        //              world_size - 1, DEGREE, DISTRIBUTION_CAP / (1024 * 1024));

        while (!no_more_tasks || tasks_outstanding > 0)
        {
            while (!idle_workers.empty() && !no_more_tasks)
            {
                TaskPackage task;
                if (!generator.next(task))
                {
                    no_more_tasks = true;
                    break;
                }
                int worker = idle_workers.front();
                idle_workers.pop();
                send_task(worker, task, MPI_COMM_WORLD);
                tasks_outstanding++;
                if (task.spill)
                    saw_spill = true;
            }

            if (tasks_outstanding == 0)
                break;

            int source_rank = 0;
            TaskPackage result = receive_result(MPI_COMM_WORLD, source_rank);
            tasks_outstanding--;
            idle_workers.push(source_rank);

            // spdlog::info("[MPI][Master] Received result from rank {} -> segment_{} slice_{} ({} items)",
            //              source_rank, result.segment_id, result.slice_index, result.items.size());

            if (result.spill)
            {
                // OOC mode: Batch DEGREE slices per segment, write once per segment
                auto &seg_info = segments[result.segment_id];
                seg_info.slices.push_back(std::move(result));

                // Check if segment is complete
                if (seg_info.slices.size() == seg_info.expected_slices)
                {
                    // spdlog::info("[MPI][Master] Segment {} complete ({} slices) - writing to disk",
                    //              result.segment_id, seg_info.slices.size());

                    // K-way merge and write segment
                    auto path = DATA_TMP_DIR + "mpi_run_" + std::to_string(run_id++) + ".bin";
                    merge_slices_to_file(seg_info.slices, path);
                    run_paths.push_back(std::move(path));
                    segments.erase(result.segment_id); // Free memory

                    // spdlog::info("[MPI][Master] Segment {} written -> {}", result.segment_id, run_paths.back());
                }
            }
            else
            {
                inmem_batches.push_back(std::make_unique<Items>(std::move(result.items)));
            }
        }

        // Handle incomplete segments (edge case)
        if (!segments.empty())
        {
            // spdlog::info("[MPI][Master] Processing {} incomplete segments", segments.size());
            for (auto &[seg_id, seg_info] : segments)
            {
                if (!seg_info.slices.empty())
                {
                    auto path = DATA_TMP_DIR + "mpi_run_" + std::to_string(run_id++) + ".bin";
                    merge_slices_to_file(seg_info.slices, path);
                    run_paths.push_back(std::move(path));
                }
            }
        }

        for (int rank = 1; rank < world_size; ++rank)
            send_terminate(rank, MPI_COMM_WORLD);

        if (!run_paths.empty())
            saw_spill = true;

        if (saw_spill)
        {
            report.METHOD = "MPI_OOC";
            finalize_spill(run_paths);
        }
        else
        {
            report.METHOD = "MPI_INMEM";
            finalize_in_memory(inmem_batches);
        }
    }

    void worker_main(int rank)
    {
        // unsigned long tid = get_tid();
        // spdlog::info("[MPI][Worker-{}] Started on TID {}", rank, tid);
        size_t task_count = 0;
        TimerClass local_timer;

        while (true)
        {
            TaskPackage task;
            if (!receive_task(0, task, MPI_COMM_WORLD))
                break;

            // spdlog::info("[MPI][Worker-{}] Processing segment_{} slice_{} ({} items) [Task #{}]",
            //              rank, task.segment_id, task.slice_index, task.items.size(), task_count + 1);

            local_timer.start();
            std::sort(task.items.begin(), task.items.end(),
                      [](const Item &a, const Item &b)
                      { return a.key < b.key; });
            local_timer.stop();

            // spdlog::info("[MPI][Worker-{}] Completed segment_{} slice_{} in {} [Task #{}]",
            //              rank, task.segment_id, task.slice_index, local_timer.result(), task_count + 1);

            send_result(0, task, MPI_COMM_WORLD);
            task_count++;
        }

        // Send worker time back to master (using a simple MPI_Send)
        uint64_t elapsed_ns = local_timer.elapsed_ns().count();
        MPI_Send(&elapsed_ns, 1, MPI_UINT64_T, 0, 999, MPI_COMM_WORLD);

        // spdlog::info("[MPI][Worker-{}] Finished: {} tasks sorted, total time {}",
        //              rank, task_count, local_timer.result());
    }
} // namespace

int main(int argc, char **argv)
{
    MPI_Init(&argc, &argv);

    int world_rank = 0;
    int world_size = 0;
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);

    try
    {
        parse_cli_and_set(argc, argv);
        if (world_rank == 0)
        {
            TimerClass total_time;
            TimerClass total_worker_time;
            {
                TimerScope total_scope(total_time);
                master_main(world_size, total_worker_time);
            }

            // Collect worker times from all workers
            uint64_t accumulated_worker_ns = 0;
            for (int rank = 1; rank < world_size; ++rank)
            {
                uint64_t worker_ns = 0;
                MPI_Recv(&worker_ns, 1, MPI_UINT64_T, rank, 999, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
                accumulated_worker_ns += worker_ns;
            }
            total_worker_time.add_elapsed(std::chrono::nanoseconds(accumulated_worker_ns));

            // Set report values
            report.WORKERS = world_size - 1; // Number of MPI workers
            report.WORKING_TIME = total_worker_time.result();
            report.TOTAL_TIME = total_time.result();

            // Final report matching ff_farm.cpp format
            spdlog::info("M: {} | R: {} | PS: {} | W: {} | DC:{}MiB | WT: {} | TT: {}",
                         report.METHOD,
                         report.RECORDS,
                         report.PAYLOAD_SIZE,
                         report.WORKERS,
                         DISTRIBUTION_CAP / IN_MB,
                         report.WORKING_TIME,
                         report.TOTAL_TIME);
        }
        else
        {
            worker_main(world_rank);
        }
    }
    catch (const std::exception &error)
    {
        spdlog::error("Rank {} aborted due to: {}", world_rank, error.what());
        MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);
    }

    MPI_Finalize();
    return EXIT_SUCCESS;
}
